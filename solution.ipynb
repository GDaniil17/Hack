{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Преобразование текстовых данных в числовой формат\u001b[39;00m\n\u001b[1;32m     16\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m---> 17\u001b[0m X_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Разделение данных на обучающий и тестовый наборы\u001b[39;00m\n\u001b[1;32m     20\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_tfidf, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/test/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2137\u001b[0m )\n\u001b[0;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/test/venv/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/test/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/test/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/PycharmProjects/test/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/PycharmProjects/test/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Загрузка данных\n",
    "data = pd.read_csv('test_data_en.csv')\n",
    "\n",
    "# Предварительная обработка данных\n",
    "data.drop(columns=['Unnamed: 0'], inplace=True)  # Удаление ненужного столбца\n",
    "X = data[['Задача en', 'Обстановка en', 'Оптимальный план en']]  # Выбор текстовых признаков\n",
    "y = data['Успех предсказанного плана']  # Целевая переменная\n",
    "\n",
    "# Преобразование текстовых данных в числовой формат\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X.values.astype('U'))\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение модели\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание на тестовом наборе\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Оценка модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.services.mts.ru/repository/pip/simple, https://artifactory.mts.ru/artifactory/api/pypi/own-onetl-pypi-local/simple, https://artifactory.mts.ru/artifactory/api/pypi/python-libs-iga-sso-pypi-local/simple/, https://artifactory.mts.ru/artifactory/mts-integration-layer-mcc-pypi-local/simple/, https://sa0000adpran:****@artifactory.mts.ru/artifactory/api/pypi/adp_ran_repo-adp-ran-pypi-local/simple/, https://minio-pre-prod.msk.bd-cloud.mts.ru\n",
      "Collecting scikit-learn\n",
      "  Downloading https://nexus.services.mts.ru/repository/pip/packages/scikit-learn/1.4.1.post1/scikit_learn-1.4.1.post1-cp39-cp39-macosx_12_0_arm64.whl (10.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.19.5 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from scikit-learn) (1.11.4)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading https://nexus.services.mts.ru/repository/pip/packages/joblib/1.3.2/joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading https://nexus.services.mts.ru/repository/pip/packages/threadpoolctl/3.4.0/threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1 threadpoolctl-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Задача en</th>\n",
       "      <th>Обстановка en</th>\n",
       "      <th>Оптимальный план en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conduct a therapy session through drawing by n...</td>\n",
       "      <td>table, paint-by-numbers kit, child, water, pap...</td>\n",
       "      <td>Find a table. Come to the table. Find the chil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prepare a salad for dinner</td>\n",
       "      <td>Kitchen, table, vegetables, meat (optional), s...</td>\n",
       "      <td>Open the door. Walk to the kitchen. Close the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cut the melon</td>\n",
       "      <td>Kitchen, table, plate, melon, knife. The melon...</td>\n",
       "      <td>Open the door. Enter the kitchen. Close the do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Production of textile products at a textile in...</td>\n",
       "      <td>Production workshop, textile materials, sewing...</td>\n",
       "      <td>Open the door. Walk to the production workshop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cutting glass for window frames at a glass fac...</td>\n",
       "      <td>Glass factory production workshop, workplace, ...</td>\n",
       "      <td>Walk to the workplace. Find a sheet of glass. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>Pour white tea into a mug.</td>\n",
       "      <td>gazebo, table, box, mug, kettle with white tea...</td>\n",
       "      <td>Find the box. Open the box. Find the mug in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>Clean the living room before guests arrive.</td>\n",
       "      <td>living room with a sofa, armchairs, coffee tab...</td>\n",
       "      <td>Open the door. Enter the living room. Close th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>Move the sofa to the window.</td>\n",
       "      <td>living room, sofa, table, window, floor. The s...</td>\n",
       "      <td>Find a sofa. Walk to the sofa. Grab the sofa. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>Organize the sorting and storage of vegetables...</td>\n",
       "      <td>Warehouse, vegetables, boxes, shelves, paper, ...</td>\n",
       "      <td>Come to the table. Find paper. Grab the paper....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>Cover the cookies with glaze.</td>\n",
       "      <td>Kitchen, table, baking sheet, cookies, bowl, i...</td>\n",
       "      <td>Come to the table. Find a bowl. Grab a bowl. F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1451 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Задача en  \\\n",
       "0     Conduct a therapy session through drawing by n...   \n",
       "1                            Prepare a salad for dinner   \n",
       "2                                         Cut the melon   \n",
       "3     Production of textile products at a textile in...   \n",
       "4     Cutting glass for window frames at a glass fac...   \n",
       "...                                                 ...   \n",
       "1446                         Pour white tea into a mug.   \n",
       "1447        Clean the living room before guests arrive.   \n",
       "1448                       Move the sofa to the window.   \n",
       "1449  Organize the sorting and storage of vegetables...   \n",
       "1450                      Cover the cookies with glaze.   \n",
       "\n",
       "                                          Обстановка en  \\\n",
       "0     table, paint-by-numbers kit, child, water, pap...   \n",
       "1     Kitchen, table, vegetables, meat (optional), s...   \n",
       "2     Kitchen, table, plate, melon, knife. The melon...   \n",
       "3     Production workshop, textile materials, sewing...   \n",
       "4     Glass factory production workshop, workplace, ...   \n",
       "...                                                 ...   \n",
       "1446  gazebo, table, box, mug, kettle with white tea...   \n",
       "1447  living room with a sofa, armchairs, coffee tab...   \n",
       "1448  living room, sofa, table, window, floor. The s...   \n",
       "1449  Warehouse, vegetables, boxes, shelves, paper, ...   \n",
       "1450  Kitchen, table, baking sheet, cookies, bowl, i...   \n",
       "\n",
       "                                    Оптимальный план en  \n",
       "0     Find a table. Come to the table. Find the chil...  \n",
       "1     Open the door. Walk to the kitchen. Close the ...  \n",
       "2     Open the door. Enter the kitchen. Close the do...  \n",
       "3     Open the door. Walk to the production workshop...  \n",
       "4     Walk to the workplace. Find a sheet of glass. ...  \n",
       "...                                                 ...  \n",
       "1446  Find the box. Open the box. Find the mug in th...  \n",
       "1447  Open the door. Enter the living room. Close th...  \n",
       "1448  Find a sofa. Walk to the sofa. Grab the sofa. ...  \n",
       "1449  Come to the table. Find paper. Grab the paper....  \n",
       "1450  Come to the table. Find a bowl. Grab a bowl. F...  \n",
       "\n",
       "[1451 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e26753e3f5a4101b3ff1f97b2e2d92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9eb2453d3f41e8855c109bd28de42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f66e65bf3194c51b4f52c97431dae1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c839b4759e843e58eb94821dcdac96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a2b5b5cd134b2d948f3c6526b4143d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6769759450171822\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Загрузка предварительно обученной модели BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Подготовка данных\n",
    "max_length = 128  # Максимальная длина последовательности\n",
    "X_tokenized = [tokenizer.encode(text, add_special_tokens=True, max_length=max_length, truncation=True) for text in X['Оптимальный план en']]\n",
    "X_padded = torch.tensor([text + [0]*(max_length - len(text)) for text in X_tokenized])\n",
    "y_tensor = torch.tensor(y.values)\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создание DataLoader для обучающего и тестового наборов\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Создание и обучение модели классификации на основе BERT\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(768, 2)  # Выходной слой для двух классов: успешно/неуспешно\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # Берем выходной вектор [CLS]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "model = BERTClassifier()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Обучение модели\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "for epoch in range(5):  # Пример: 5 эпох\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, labels = batch\n",
    "        attention_mask = (input_ids != 0).float()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Оценка модели\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "for batch in test_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, labels = batch\n",
    "    attention_mask = (input_ids != 0).float()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "    predictions.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# алгоритмические сравнения, catboost's, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nexus.services.mts.ru/repository/pip/simple, https://artifactory.mts.ru/artifactory/api/pypi/own-onetl-pypi-local/simple, https://artifactory.mts.ru/artifactory/api/pypi/python-libs-iga-sso-pypi-local/simple/, https://artifactory.mts.ru/artifactory/mts-integration-layer-mcc-pypi-local/simple/, https://sa0000adpran:****@artifactory.mts.ru/artifactory/api/pypi/adp_ran_repo-adp-ran-pypi-local/simple/, https://minio-pre-prod.msk.bd-cloud.mts.ru\n",
      "Requirement already satisfied: transformers in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (4.39.3)\n",
      "Collecting torch\n",
      "  Downloading https://nexus.services.mts.ru/repository/pip/packages/torch/2.2.2/torch-2.2.2-cp39-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m893.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from torch) (4.9.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading https://nexus.services.mts.ru/repository/pip/packages/sympy/1.12/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting networkx (from torch)\n",
      "  Downloading https://nexus.services.mts.ru/repository/pip/packages/networkx/3.2.1/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m863.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aatara57/PycharmProjects/test/venv/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading https://nexus.services.mts.ru/repository/pip/packages/mpmath/1.3.0/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m888.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
